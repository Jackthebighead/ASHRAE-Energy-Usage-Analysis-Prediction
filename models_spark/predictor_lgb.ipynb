{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGB_1: An Implementation of Sample_11\n",
    "`@Author: YUAN Yanzhe`  \n",
    "The 3rd iteration    \n",
    "Use lgb model, refered to `Sample_11`\n",
    "\n",
    "Output: ashrae_lgb_1.csv\n",
    "\n",
    "Scores: 1.135"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yanzheyuan/study/hkust_bdt/courses/5003_bigdatacomp/proj/codes\n",
      "/Users/yanzheyuan/study/hkust_bdt/courses/5003_bigdatacomp/proj/codes\n",
      "/Users/yanzheyuan/study/hkust_bdt/courses/5003_bigdatacomp/proj\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "print(os.getcwd())\n",
    "print(sys.path[0])\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 73.8%\n"
     ]
    }
   ],
   "source": [
    "df_building = reduce_mem_usage(pd.read_csv('ashrae-energy-prediction/building_metadata.csv'))\n",
    "df_building['primary_use'] = LabelEncoder().fit_transform(df_building.primary_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 616.95 MB\n",
      "Memory usage after optimization is: 173.90 MB\n",
      "Decreased by 71.8%\n",
      "Memory usage of dataframe is 9.60 MB\n",
      "Memory usage after optimization is: 2.65 MB\n",
      "Decreased by 72.4%\n",
      "Memory usage of dataframe is 2690.83 MB\n",
      "Memory usage after optimization is: 1099.07 MB\n",
      "Decreased by 59.2%\n"
     ]
    }
   ],
   "source": [
    "# load csv files\n",
    "df_meter = reduce_mem_usage(pd.read_csv('ashrae-energy-prediction/train.csv'))\n",
    "df_weather = reduce_mem_usage(pd.read_csv('ashrae-energy-prediction/weather_train.csv'))\n",
    "\n",
    "# create a dense timestamp (bcz weather's is sparse)\n",
    "timestamp = df_meter.timestamp.unique()\n",
    "timestamp = np.sort(timestamp)\n",
    "timestamp = pd.DataFrame(data=timestamp, columns=['timestamp'])\n",
    "\n",
    "dt = pd.DatetimeIndex(timestamp.timestamp)\n",
    "timestamp['day'] = dt.day\n",
    "timestamp['hour'] = dt.hour\n",
    "timestamp['weekday'] = dt.weekday\n",
    "\n",
    "# project each site's weather data to the dense timestamp and do interpolation\n",
    "dfs = []\n",
    "for idx, group in df_weather.groupby('site_id'):\n",
    "    group = pd.merge(timestamp, group, on='timestamp', how='left')\n",
    "    group = group.interpolate(limit_direction='both')\n",
    "    group['air_temperature_24'] = group.air_temperature.rolling(24).mean().fillna(method='bfill')\n",
    "    group['air_temperature_48'] = group.air_temperature.rolling(48).mean().fillna(method='bfill')\n",
    "    group['air_temperature_96'] = group.air_temperature.rolling(96).mean().fillna(method='bfill')\n",
    "    group['dew_temperature_24'] = group.dew_temperature.rolling(24).mean().fillna(method='bfill')\n",
    "    group['dew_temperature_48'] = group.dew_temperature.rolling(48).mean().fillna(method='bfill')\n",
    "    group['dew_temperature_96'] = group.dew_temperature.rolling(96).mean().fillna(method='bfill')\n",
    "    group['sea_level_pressure_24'] = group.sea_level_pressure.rolling(24).mean().fillna(method='bfill')\n",
    "    group['sea_level_pressure_48'] = group.sea_level_pressure.rolling(48).mean().fillna(method='bfill')\n",
    "    group['sea_level_pressure_96'] = group.sea_level_pressure.rolling(96).mean().fillna(method='bfill')\n",
    "    dfs.append(group)\n",
    "df_weather = pd.concat(dfs)\n",
    "\n",
    "# merge meter data with the building data\n",
    "df = pd.merge(df_meter, df_building, on='building_id', how='left')\n",
    "\n",
    "# merge meter data with the weather data\n",
    "df = pd.merge(df, df_weather, on=['site_id', 'timestamp'], how='left')\n",
    "\n",
    "# mathmatical conversion\n",
    "df['meter_reading'] = np.log1p(df.meter_reading)\n",
    "df['wind_direction'] = np.round(df.wind_direction, -1)\n",
    "\n",
    "# reduce memory size\n",
    "where = df.meter == 0\n",
    "where &= df.site_id == 0\n",
    "where &= df.timestamp < '2016-05-20'\n",
    "df.drop(index=df[where].index, inplace=True)\n",
    "df.drop(columns='timestamp', inplace=True)\n",
    "df = reduce_mem_usage(df)\n",
    "\n",
    "# save to the disk\n",
    "df.to_pickle('train.pickle')\n",
    "\n",
    "# Collect memory\n",
    "del df, dfs, idx, group, df_meter, df_weather, timestamp, dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1272.51 MB\n",
      "Memory usage after optimization is: 358.65 MB\n",
      "Decreased by 71.8%\n",
      "Memory usage of dataframe is 19.04 MB\n",
      "Memory usage after optimization is: 5.25 MB\n",
      "Decreased by 72.4%\n",
      "Memory usage of dataframe is 5646.76 MB\n",
      "Memory usage after optimization is: 2385.96 MB\n",
      "Decreased by 57.7%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## load csv files\n",
    "df_meter = reduce_mem_usage(pd.read_csv('ashrae-energy-prediction/test.csv'))\n",
    "df_weather = reduce_mem_usage(pd.read_csv('ashrae-energy-prediction/weather_test.csv'))\n",
    "\n",
    "# create a dense timestamp (bcz weather's is sparse)\n",
    "timestamp = df_meter.timestamp.unique()\n",
    "timestamp = np.sort(timestamp)\n",
    "timestamp = pd.DataFrame(data=timestamp, columns=['timestamp'])\n",
    "\n",
    "dt = pd.DatetimeIndex(timestamp.timestamp)\n",
    "timestamp['day'] = dt.day\n",
    "timestamp['hour'] = dt.hour\n",
    "timestamp['weekday'] = dt.weekday\n",
    "\n",
    "# project each site's weather data to the dense timestamp and do interpolation\n",
    "dfs = []\n",
    "for idx, group in df_weather.groupby('site_id'):\n",
    "    group = pd.merge(timestamp, group, on='timestamp', how='left')\n",
    "    group = group.interpolate(limit_direction='both')\n",
    "    group['air_temperature_24'] = group.air_temperature.rolling(24).mean().fillna(method='bfill')\n",
    "    group['air_temperature_48'] = group.air_temperature.rolling(48).mean().fillna(method='bfill')\n",
    "    group['air_temperature_96'] = group.air_temperature.rolling(96).mean().fillna(method='bfill')\n",
    "    group['dew_temperature_24'] = group.dew_temperature.rolling(24).mean().fillna(method='bfill')\n",
    "    group['dew_temperature_48'] = group.dew_temperature.rolling(48).mean().fillna(method='bfill')\n",
    "    group['dew_temperature_96'] = group.dew_temperature.rolling(96).mean().fillna(method='bfill')\n",
    "    group['sea_level_pressure_24'] = group.sea_level_pressure.rolling(24).mean().fillna(method='bfill')\n",
    "    group['sea_level_pressure_48'] = group.sea_level_pressure.rolling(48).mean().fillna(method='bfill')\n",
    "    group['sea_level_pressure_96'] = group.sea_level_pressure.rolling(96).mean().fillna(method='bfill')\n",
    "    dfs.append(group)\n",
    "df_weather = pd.concat(dfs)\n",
    "\n",
    "# merge meter data with the building data\n",
    "df = pd.merge(df_meter, df_building, on='building_id', how='left')\n",
    "\n",
    "# merge meter data with the weather data\n",
    "df = pd.merge(df, df_weather, on=['site_id', 'timestamp'], how='left')\n",
    "\n",
    "# mathmatical conversion\n",
    "df['wind_direction'] = np.round(df.wind_direction, -1)\n",
    "\n",
    "# reduce memory size\n",
    "df.drop(columns='timestamp', inplace=True)\n",
    "df = reduce_mem_usage(df)\n",
    "\n",
    "# save to the disk\n",
    "df.to_pickle('test.pickle')\n",
    "\n",
    "# Collect memory\n",
    "del df, dfs, idx, group, df_meter, df_weather, timestamp, dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/software/anaconda/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('train.pickle')\n",
    "X = df.drop(columns='meter_reading')\n",
    "y = df.meter_reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(4)\n",
    "scores = {'estimator': [], 'train_score': [], 'test_score': []}\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': {'rmse'},\n",
    "    'num_leaves': 50,\n",
    "    'bagging_fraction': 0.1,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'learning_rate': 0.2,\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'seed': 0\n",
    "}\n",
    "\n",
    "for train, valid in cv.split(X, y):\n",
    "    gc.collect()\n",
    "    \n",
    "    X_train, y_train = X.iloc[train], y.iloc[train]\n",
    "    X_valid, y_valid = X.iloc[valid], y.iloc[valid]\n",
    "    \n",
    "    train = lgb.Dataset(X_train, y_train)\n",
    "    valid = lgb.Dataset(X_valid, y_valid)\n",
    "    \n",
    "    estimator = lgb.train(\n",
    "        params=params,\n",
    "        train_set=train,\n",
    "        valid_sets=(train, valid),\n",
    "        num_boost_round=1000,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    scores['estimator'].append(estimator)\n",
    "    scores['train_score'].append(estimator.best_score['training']['rmse'])\n",
    "    scores['test_score'].append(estimator.best_score['valid_1']['rmse'])\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['train_score', 'test_score']\n",
    "scores = pd.DataFrame(scores)\n",
    "scores[cols] = scores[cols]\n",
    "scores[cols].plot(kind='bar')\n",
    "print(scores[cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = [model.feature_importance() for model in scores['estimator']]\n",
    "feature_importances = np.mean(feature_importances, axis=0)\n",
    "feature_importances = pd.DataFrame(data=feature_importances, index=X.columns, columns=['feature_importance'])\n",
    "feature_importances.sort_values('feature_importance', ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=[12, 8])\n",
    "sns.barplot(x=feature_importances.feature_importance, y=feature_importances.index, orient='h')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(scores['estimator']):\n",
    "    model.save_model('{}.model'.format(idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('test.pickle')\n",
    "df = df.sort_values('row_id')\n",
    "df = df.drop(columns='row_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lgb.Booster(model_file='{}.model'.format(i)) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inference(models, df, n_batch):\n",
    "    \n",
    "    result = []\n",
    "    n_batch = int(n_batch)\n",
    "    \n",
    "    for idx in np.arange(0, df.shape[0], n_batch):\n",
    "        progress = idx / df.shape[0] * 100\n",
    "        progress = np.round(progress, 2)\n",
    "        print('\\r', progress, end='')\n",
    "        \n",
    "        start = idx\n",
    "        end = (idx + n_batch)\n",
    "        batch = df[start:end]\n",
    "                \n",
    "        pred = [model.predict(batch) for model in models]\n",
    "        pred = np.mean(pred, axis=0)\n",
    "        \n",
    "        result.append(pred)\n",
    "        \n",
    "    result = np.concatenate(result)\n",
    "    \n",
    "    print('\\r', '100.00')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = batch_inference(models, df, 1e4)\n",
    "y_pred = np.clip(y_pred, 0, None)\n",
    "y_pred = np.expm1(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "print(os.getcwd())\n",
    "print(sys.path[0])\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "print(parent_dir)\n",
    "df = pd.read_csv(parent_dir+'/ashrae-energy-prediction/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meter_reading'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ashrae_lgb_1_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I need to 整理 this codes as a lgb baseline cause it is the best score till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
